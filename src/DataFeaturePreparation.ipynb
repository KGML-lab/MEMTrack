{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d3e63a-371e-4751-a446-0a7152844814",
   "metadata": {},
   "source": [
    "# Generate Features for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8fe4e0-0ab9-49a1-8d44-5e8d4215fea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1694965-28cb-4050-af0b-d02da2ff8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import openpyxl\n",
    "import os\n",
    "import tqdm\n",
    "import csv\n",
    "import cv2\n",
    "import shutil\n",
    "import PIL\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ffedb-f022-4519-a93e-059b10d103ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250895aa-6108-415e-9ce6-70a09f1ab336",
   "metadata": {},
   "source": [
    "Update the variables in the next cell according to your directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce49ad1f-e421-45a7-8b61-963072c9f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data_sub_dir = \"/home/medha/BacteriaDetectionTracking/MEMTrack/data/collagen/\" #target sub data dir\n",
    "video_map_path = \"/home/medha/BacteriaDetectionTracking/MEMTrack/data/videomap.txt\" #path to video map\n",
    "\n",
    "# data_labels_exist_easy = 0\n",
    "# data_labels_exist_hard = 1\n",
    "# data_labels_exist_veryhard = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda9f91c-0170-4c46-8fc6-b07d78d5eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(data_dir):\n",
    "    # choose codec according to format needed\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    #print(data_dir)\n",
    "    img_sample = cv2.imread(os.path.join(data_dir,\"images/0.tif\"))\n",
    "    #print(img_sample.shape)\n",
    "    height, width, channels = img_sample.shape\n",
    "    \n",
    "    video = cv2.VideoWriter(data_dir + 'video.mp4', fourcc, 1, (width, height))\n",
    "    #data_dir = \"./Data/video3/\"\n",
    "    image_dir = os.path.join(data_dir, \"images\")\n",
    "    for frame in natsorted(os.listdir(image_dir)):\n",
    "        #print(frame)\n",
    "        img = cv2.imread(os.path.join(image_dir, frame))\n",
    "        video.write(img)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a00871-ed40-46e7-9fde-1d9133a5a63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterate through video1,2,4\n",
    "#create videos for all frame sets- 1-1,1-2..2-1..4-2\n",
    "# data_path = target_data_sub_dir\n",
    "# test_video = [f\"video{57}\"]\n",
    "# for video in natsorted(test_video):\n",
    "#     if not video.startswith('.') and os.path.isdir(os.path.join(data_path,video))==True:\n",
    "#         for minivideo in natsorted(os.listdir(os.path.join(data_path,video))):\n",
    "#             if not minivideo.startswith('.') and os.path.isdir(os.path.join(data_path,video,minivideo)) == True:\n",
    "#                 #print(minivideo)\n",
    "#                 create_video(os.path.join(data_path,video,minivideo))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8fe176-24d0-4ed9-b79b-48e6e6bbf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_background(file_path, mean=True, sample=False):\n",
    "    if not os.path.exists(file_path):\n",
    "        create_video(file_path.rsplit(\"/\",1)[0] +\"/frame1\")\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    #print(cap.read())\n",
    "    # we will randomly select 50 frames for the calculating the median\n",
    "    #frame_indices = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=20)\n",
    "    frame_indices = list(range(0,int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) ))\n",
    "    print(len(frame_indices))\n",
    "    # we will store the frames in array\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        # set the frame id to read that particular frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        #print(ret)\n",
    "        frames.append(frame)\n",
    "    if mean:\n",
    "         # calculate the mean\n",
    "        background_frame = np.mean(frames, axis=0).astype(np.uint8)\n",
    "    else:\n",
    "        # calculate the median\n",
    "        background_frame = np.median(frames, axis=0).astype(np.uint8)\n",
    "    if sample==True:\n",
    "        background_frame = cv2.imread(\"./Control_2_b0t5306c0x0-660y0-492.tiff\")\n",
    "        #background_frame = cv2.imread(\"./RBS 2_1_b0t2791c0x0-660y0-492.tiff\")\n",
    "    return background_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab745de-2715-4f7e-b7af-069027cb128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(492, 660, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_background(f\"{target_data_sub_dir}/video{1}/frame1video.mp4\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cedf537d-c436-4f73-93ad-8c3b71e30b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from numpy import *\n",
    "# background_img = PIL.Image.fromarray(get_background(f\"{target_data_sub_dir}/video{29}/frame1video.mp4\"))\n",
    "# frame_img = Image.open(f\"{target_data_sub_dir}/video{29}/frame1/images/13.tif\")\n",
    "# #Image.fromarray(get_background(f\"{target_data_sub_dir}/video{30}/frame1video.mp4\"), \"RGB\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540ca28d-7e72-44d8-91a0-9ab758a28792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_diff = PIL.ImageChops.subtract(frame_img, background_img)\n",
    "# PIL.ImageChops.subtract(background_img, frame_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbebe3-e872-4afe-8e62-0d5729134d66",
   "metadata": {},
   "source": [
    "# Feature Generation Code for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675e643a-0579-4909-bc0f-ea713a1277a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_absolute_consec_frame_diff_feature(video_path, max_consec_frame_diff=True, num_prior_frames=1):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frames = []\n",
    "#     differences = []\n",
    "#     prev = None\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         #print(frame)\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if prev is None:\n",
    "#             #print(prev)\n",
    "#             prev = frame\n",
    "#             continue\n",
    "#         #computing frame diff between current and one previous\n",
    "#         consecutive_diff = np.abs(frame - prev)\n",
    "#         frames.append(frame) #creating frame list\n",
    "#         #creating consecutive frame diff list\n",
    "#         differences.append(consecutive_diff)\n",
    "#     #creating consecutive frame diff features by taking the max at every pixel along the frame diff list\n",
    "#     if max_consec_frame_diff:\n",
    "#         max_abs_consec_diff_feature = np.max(differences, axis=0)\n",
    "#     else:\n",
    "#          max_abs_consec_diff_feature = np.min(differences, axis=0)\n",
    "#     return max_abs_consec_diff_feature\n",
    "\n",
    "def get_absolute_consec_frame_diff_feature(video_path, max_consec_frame_diff=True, num_prior_frames=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    differences = []\n",
    "    prev_frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        #print(frame)\n",
    "        if not ret:\n",
    "            break\n",
    "        #storing old frames till prior frame number\n",
    "        if num_prior_frames>0:\n",
    "            prev_frames.append(frame)\n",
    "            num_prior_frames -= 1\n",
    "            continue\n",
    "        #retrieving the previous xth frame\n",
    "        prev = prev_frames[0]\n",
    "        prev_frames.pop(0)\n",
    "        #computing frame diff between current and one previous\n",
    "        consecutive_diff = np.abs(frame - prev)\n",
    "        frames.append(frame) #creating frame list\n",
    "        #creating consecutive frame diff list\n",
    "        differences.append(consecutive_diff)\n",
    "    #creating consecutive frame diff features by taking the max at every pixel along the frame diff list\n",
    "    if max_consec_frame_diff:\n",
    "        max_abs_consec_diff_feature = np.max(differences, axis=0)\n",
    "    else:\n",
    "         max_abs_consec_diff_feature = np.min(differences, axis=0)\n",
    "    return max_abs_consec_diff_feature\n",
    "    \n",
    "def get_diff_from_absolute_consec_frame_diff_feature(image_path, frame_diff_feature, frame=False):\n",
    "    if frame==False:\n",
    "        image = PIL.Image.open(image_path).convert('L')\n",
    "    if frame == True:\n",
    "        #print(\"getting diff from frame\")\n",
    "        image = Image.fromarray(image_path).convert('L')\n",
    "    L = image.getchannel(0)\n",
    "    frame_diff_feature = PIL.Image.fromarray(frame_diff_feature).convert('L')\n",
    "    frame_diff_feature.getbands()\n",
    "    L1 = frame_diff_feature.getchannel(0)\n",
    "    diff = PIL.ImageChops.difference(L, L1)\n",
    "    return diff\n",
    "\n",
    "def get_absolute_all_frame_diff_feature(video_path, max_feature=True):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        #print(frame)\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    features = []\n",
    "    count = 0\n",
    "    for index in tqdm.tqdm(range(len(frames))):\n",
    "        differences = []\n",
    "        count+=1\n",
    "        frame = frames[index]\n",
    "        for index_1 in (range(len(frames))):\n",
    "            if index == index_1:\n",
    "                #print(count)\n",
    "                continue\n",
    "            frame1 = frames[index_1]\n",
    "            differences.append(np.abs(frame - frame1))\n",
    "        if max_feature:\n",
    "            max_diff_feature = np.max(differences, axis=0)\n",
    "            features.append(max_diff_feature)\n",
    "        else:\n",
    "            #print(\"min\")\n",
    "            min_diff_feature = np.min(differences, axis=0)\n",
    "            features.append(min_diff_feature)\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8135f7a-cf4b-47df-a3f0-c62efd74b4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def gen_dense_optical_flow_data(method, video_path, params=[], to_gray=False, median=False, median_frame=None, num_frames_prior=1):\n",
    "#     frames_optical_flow = []\n",
    "#     frames_orignal = []\n",
    "#     # Read the video and first frame\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     #print(\"fps\",cap.get(cv2.CAP_PROP_FPS))\n",
    "#     ret, old_frame = cap.read()\n",
    "\n",
    "#     if median == True:\n",
    "#         old_frame = median_frame\n",
    "#         old_frame = cv2.cvtColor(old_frame, cv2.COLOR_GRAY2BGR)\n",
    "#     # crate HSV & make Value a constant\n",
    "#     hsv = np.zeros_like(old_frame)\n",
    "#     hsv[..., 1] = 255\n",
    "\n",
    "#     # Preprocessing for exact method\n",
    "#     if to_gray:\n",
    "#         old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "#     while True:\n",
    "#         # Read the next frame\n",
    "#         ret, new_frame = cap.read()\n",
    "#         frame_copy = new_frame\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # Preprocessing for exact method\n",
    "#         if to_gray:\n",
    "#             new_frame = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         # Calculate Optical Flow\n",
    "#         flow = method(old_frame, new_frame, None, *params)\n",
    "\n",
    "#         # Encoding: convert the algorithm's output into Polar coordinates\n",
    "#         mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "#         # Use Hue and Value to encode the Optical Flow\n",
    "#         hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "#         hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "#         # Convert HSV image into BGR for demo\n",
    "#         bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "#         #plt.imshow(frame_copy)\n",
    "#         #plt.imshow(bgr)\n",
    "#         # plt.imshow(v,cmap='gray', vmin=0, vmax=255)\n",
    "#         # plt.show()\n",
    "    \n",
    "#         # Update the previous frame\n",
    "#         old_frame = new_frame\n",
    "#         if median == True:\n",
    "#             old_frame = median_frame\n",
    "#         frames_orignal.append(frame_copy)\n",
    "#         frames_optical_flow.append(bgr)\n",
    "#     return frames_orignal, frames_optical_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123df99d-becf-421e-9091-595be479f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated to include optical floiw computtaion from x frames prior\n",
    "def gen_dense_optical_flow_data(method, video_path, params=[], to_gray=False, median=False, median_frame=None, num_frames_prior=1):\n",
    "    frames_optical_flow = []\n",
    "    frames_orignal = []\n",
    "    # Read the video and first x frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    #print(\"fps\",cap.get(cv2.CAP_PROP_FPS))\n",
    "    old_frames = []\n",
    "    for i in range(num_frames_prior):\n",
    "        ret, old_frame = cap.read()\n",
    "        # crate HSV & make Value a constant\n",
    "        hsv = np.zeros_like(old_frame)\n",
    "        hsv[..., 1] = 255\n",
    "        if to_gray:\n",
    "            old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "        old_frames.append(old_frame)\n",
    "\n",
    "    #to compute optical flow from the median background    \n",
    "    if median == True:\n",
    "        old_frame = median_frame\n",
    "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "  \n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, new_frame = cap.read()\n",
    "        frame_copy = new_frame\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        old_frame = old_frames[0]\n",
    "        if median == True:\n",
    "            old_frame = median_frame\n",
    "\n",
    "        # Preprocessing for exact method\n",
    "        if to_gray:\n",
    "            new_frame = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "            \n",
    "        # Calculate Optical Flow\n",
    "      \n",
    "        flow = method(old_frame, new_frame, None, *params)\n",
    "\n",
    "        # Encoding: convert the algorithm's output into Polar coordinates\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        # Use Hue and Value to encode the Optical Flow\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Convert HSV image into BGR for demo\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        #plt.imshow(frame_copy)\n",
    "        #plt.imshow(bgr)\n",
    "        # plt.imshow(v,cmap='gray', vmin=0, vmax=255)\n",
    "        # plt.show()\n",
    "    \n",
    "        # Update the previous frame\n",
    "        old_frames.append(new_frame)\n",
    "        old_frames.pop(0)\n",
    "        frames_orignal.append(frame_copy)\n",
    "        frames_optical_flow.append(bgr)\n",
    "    return frames_orignal, frames_optical_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee71c42-d2b2-4bc5-a6cd-dcd9a4b07c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_background_diff_feature(image_path, background):\n",
    "    image = PIL.Image.open(image_path).convert('L')\n",
    "    L = image.getchannel(0)\n",
    "    background_img = PIL.Image.fromarray(background)\n",
    "    background_img.getbands()\n",
    "    L1 = background_img.getchannel(0)\n",
    "    diff = PIL.ImageChops.difference(L, L1)\n",
    "    return diff\n",
    "    \n",
    "def get_prev_frame_diff_feature(image_path, prev_image_path):\n",
    "    image = PIL.Image.open(image_path).convert('L')\n",
    "    L = image.getchannel(0)\n",
    "    image = PIL.Image.open(prev_image_path).convert('L')\n",
    "    L1 = image.getchannel(0)\n",
    "    diff = PIL.ImageChops.difference(L, L1)\n",
    "    return diff\n",
    "\n",
    "def create_feature_image(image_path, background, prev_image_path, prev_image_path2=None):\n",
    "    prev_image_diff = get_prev_frame_diff_feature(image_path, prev_image_path)\n",
    "    image = PIL.Image.open(image_path).convert('L')\n",
    "    L = image.getchannel(0)\n",
    "    if prev_image_path2 is None:\n",
    "        background_diff = get_background_diff_feature(image_path, background)\n",
    "        newImagediff = PIL.Image.merge(\"RGB\", [L, background_diff, prev_image_diff])\n",
    "    if prev_image_path2 is not None:\n",
    "        prev_image_diff2 = get_prev_frame_diff_feature(image_path, prev_image_path2)\n",
    "        newImagediff = PIL.Image.merge(\"RGB\", [L, prev_image_diff2, prev_image_diff])\n",
    "        \n",
    "    return newImagediff\n",
    "\n",
    "def create_feature_image_optical_flow(frame, optical_flow, pure=False, background=None, optical_flow2=None, final_channel=False):\n",
    "    frame = Image.fromarray(frame).convert('L')\n",
    "    L = frame.getchannel(0)\n",
    "    flow = PIL.Image.fromarray(optical_flow)\n",
    "    #flow.save(\"flow.png\")\n",
    "    hsv_optical_flow = cv2.cvtColor(optical_flow, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv_optical_flow)\n",
    "    flow = PIL.Image.fromarray(v)\n",
    "    #flow.save(\"flow_hsv.png\")\n",
    "    \n",
    "    #print(v.shape)\n",
    "    last_channel = np.zeros([v.shape[0], v.shape[1]],dtype=np.uint8)\n",
    "    last_channel[:] = 255\n",
    "    v = Image.fromarray(v).convert('L')\n",
    "    v = v.getchannel(0)\n",
    "    \n",
    "    \n",
    "    #print(last_channel)\n",
    "    last_channel = Image.fromarray(last_channel).convert('L')\n",
    "    last_channel = last_channel.getchannel(0)\n",
    "\n",
    "    gray_optical_flow = Image.fromarray(cv2.cvtColor(optical_flow, cv2.COLOR_BGR2GRAY)).convert('L').getchannel(0)\n",
    "    \n",
    "    feature_image = PIL.Image.merge(\"RGB\", [L, v, last_channel])\n",
    "    feature_image = PIL.Image.merge(\"RGB\", [L, gray_optical_flow, last_channel])\n",
    "    \n",
    "    if pure == False:\n",
    "        if background is None:\n",
    "            gray_optical_flow2 = Image.fromarray(cv2.cvtColor(optical_flow2, cv2.COLOR_BGR2GRAY)).convert('L').getchannel(0)\n",
    "            feature_image = PIL.Image.merge(\"RGB\", [L, gray_optical_flow, gray_optical_flow2])\n",
    "            #print(here)\n",
    "        else:\n",
    "            if final_channel == False:\n",
    "                background_img = PIL.Image.fromarray(background)\n",
    "                L1 = background_img.getchannel(0)\n",
    "                feature_image = PIL.Image.merge(\"RGB\", [L, v, L1]) #-->org code\n",
    "                # diff = PIL.ImageChops.difference(L, L1)\n",
    "                # feature_image = PIL.Image.merge(\"RGB\", [L, v, diff]) #-->up code\n",
    "                #print(\"here adding background\")\n",
    "                #print(\"background\")\n",
    "            if final_channel==True:\n",
    "                #print(\"final channel tru, just adding diff\")\n",
    "                feature_image = PIL.Image.merge(\"RGB\", [L, v, background]) \n",
    "            \n",
    "       \n",
    "        \n",
    "    # print(feature_image.size)\n",
    "    # print(feature_image.mode)\n",
    "        \n",
    "    return feature_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b07b83d-077f-4a0b-aaae-3f4e96503ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(image_dest, count, feature_image, frame_num, data_dir, video, minivideo, count_test, count_train, train):\n",
    "    #save image feature\n",
    "    feature_image.save(image_dest +\"images_feature/\"+str(count)+\".tif\")\n",
    "\n",
    "    text_file = str(frame_num) +\".txt\"\n",
    "    annotations_easy_source = os.path.join(data_dir, video, minivideo, \"annotations_easy\",\n",
    "                                           text_file)\n",
    "    annotations_easy_hard_source = os.path.join(data_dir, video, minivideo, \n",
    "                                            \"annotations_easy_hard\", text_file)\n",
    "    \n",
    "    annotations_very_hard_source = os.path.join(data_dir, video, minivideo, \"annotations_veryhard\",\n",
    "                                           text_file)\n",
    "    annotations_easy_hard_veryhard_source = os.path.join(data_dir, video, minivideo, \n",
    "                                            \"annotations_easy_hard_veryhard\", text_file)\n",
    "\n",
    "\n",
    "    shutil.copy(annotations_easy_source, image_dest +\"annotation_easy/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_easy_hard_source, image_dest +\"annotation_easy_hard/\" \n",
    "                + str(count)+\".txt\")\n",
    "    if os.path.exists(annotations_very_hard_source):\n",
    "        shutil.copy(annotations_very_hard_source, image_dest +\"annotation_veryhard/\" +str(count)+\".txt\")\n",
    "    if os.path.exists(annotations_easy_hard_veryhard_source):\n",
    "        shutil.copy(annotations_easy_hard_veryhard_source, image_dest +\"annotation_easy_hard_veryhard/\" \n",
    "                + str(count)+\".txt\")\n",
    "    #create hard only annotation\n",
    "    text_file_easy_hard = open(annotations_easy_hard_source, 'r')\n",
    "    xy_coords_easy_hard  = text_file_easy_hard.readlines()\n",
    "\n",
    "    text_file_easy = open(annotations_easy_source, 'r')\n",
    "    xy_coords_easy  = text_file_easy.readlines()\n",
    "\n",
    "    xy_coords_hard = [coord for coord in xy_coords_easy_hard if coord not in xy_coords_easy ] \n",
    "    text_file_hard = open(image_dest +\"annotation_hard/\" +str(count)+\".txt\", 'w')\n",
    "    for coord in xy_coords_hard:\n",
    "        text_file_hard.write(coord)\n",
    "    text_file_hard.close()\n",
    "\n",
    "    annotations_low_motility_source = os.path.join(data_dir, video, minivideo, \"annotations_motility_low\",\n",
    "                                           text_file)\n",
    "    annotations_high_motility_source = os.path.join(data_dir, video, minivideo, \"annotations_motility_high\",\n",
    "                                           text_file)\n",
    "    annotations_mid_motility_source = os.path.join(data_dir, video, minivideo, \"annotations_motility_mid\",\n",
    "                                           text_file)\n",
    "    annotations_wiggle_motility_source = os.path.join(data_dir, video, minivideo, \"annotations_motility_wiggle\",\n",
    "                                           text_file)\n",
    "    \n",
    "    annotations_sticking_motile_source = os.path.join(data_dir, video, minivideo, \"annotations_sticking_motile\",\n",
    "                                           text_file)\n",
    "    annotations_sticking_stick_source = os.path.join(data_dir, video, minivideo, \"annotations_sticking_stick\",\n",
    "                                           text_file)\n",
    "    annotations_sticking_non_motile_source = os.path.join(data_dir, video, minivideo, \"annotations_sticking_non_motile\",\n",
    "                                           text_file)\n",
    "    \n",
    "    shutil.copy(annotations_low_motility_source, image_dest +\"annotation_motility_low/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_high_motility_source, image_dest +\"annotation_motility_high/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_mid_motility_source, image_dest +\"annotation_motility_mid/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_wiggle_motility_source, image_dest +\"annotation_motility_wiggle/\" +str(count)+\".txt\")\n",
    "    \n",
    "    shutil.copy(annotations_sticking_stick_source, image_dest +\"annotation_sticking_stick/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_sticking_motile_source, image_dest +\"annotation_sticking_motile/\" +str(count)+\".txt\")\n",
    "    shutil.copy(annotations_sticking_non_motile_source, image_dest +\"annotation_sticking_non_motile/\" +str(count)+\".txt\")\n",
    "    \n",
    "    \n",
    "    if train == True:\n",
    "        count_train += 1\n",
    "    else:\n",
    "        count_test += 1\n",
    "        \n",
    "    return count_test, count_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b476b9-9f1a-46f6-bafc-60b38eb93197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get backgorund frame for every mini video\n",
    "#skip the first frame in every mini video\n",
    "#store image in train images set\n",
    "#store image+background diff + prev image diff in train images feature set\n",
    "#similarly for test\n",
    "\n",
    "def create_data(data_dir, dest_dir, trainfolder, train_video, testfolder, test_video, valfolder, val_video, method=\"background\", num_prev=None, mean=False, sample=False, test_only = False, params=None, optical_flow_prior=1, frame_diff_prior=1):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    data_dir_types = [\"/images/\", \"/images_feature/\", \"/annotation_easy/\", \"/annotation_hard/\", \"/annotation_easy_hard/\", \"/annotation_easy_hard_veryhard/\" , \"/annotation_veryhard/\" , \"/annotation_motility_low\", \"/annotation_motility_wiggle\", \"/annotation_motility_mid\", \"/annotation_motility_high\", \"/annotation_sticking_stick\", \"/annotation_sticking_motile\", \"/annotation_sticking_non_motile\"]\n",
    "    for video in test_video:\n",
    "        for dir_type in data_dir_types:\n",
    "            os.makedirs(os.path.join(dest_dir, testfolder, video) + dir_type, exist_ok=True)\n",
    "            \n",
    "    for video in val_video:\n",
    "        for dir_type in data_dir_types:\n",
    "            if test_only == True:\n",
    "                continue\n",
    "            os.makedirs(os.path.join(dest_dir, valfolder, video) + dir_type, exist_ok=True)\n",
    "            \n",
    "    for dir_type in data_dir_types:\n",
    "            os.makedirs(os.path.join(dest_dir, testfolder) + dir_type, exist_ok=True)\n",
    "            if test_only == True:\n",
    "                continue\n",
    "            os.makedirs(os.path.join(dest_dir, valfolder) + dir_type, exist_ok=True)\n",
    "            os.makedirs(dest_dir+trainfolder + dir_type, exist_ok=True)\n",
    "    \n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    count_test_all = 0\n",
    "    count_val_all = 0\n",
    "\n",
    "    for video in natsorted(os.listdir(data_dir)):\n",
    "        \n",
    "        if not video.startswith('.') and os.path.isdir(os.path.join(data_dir,video))==True:\n",
    "            if test_only == True and video not in test_video:\n",
    "                continue\n",
    "            if video not in train_video + test_video + val_video:\n",
    "                continue\n",
    "            for minivideo in natsorted(os.listdir(os.path.join(data_dir,video))) :\n",
    "                if not minivideo.startswith('.') and os.path.isdir(os.path.join(data_dir,video,minivideo)) == True:\n",
    "                    video_path = os.path.join(data_dir,video) + \"/\" + minivideo + \"video.mp4\"\n",
    "                    print(video_path)\n",
    "\n",
    "                    if method == \"background\":\n",
    "                        # get the background model\n",
    "                        background = get_background(video_path, mean, sample=sample)\n",
    "                        # convert the background model to grayscale format\n",
    "                        background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "                        skip_frame_list = [0]\n",
    "                    if method == \"nprev\":\n",
    "                        skip_frame_list = list(range(num_prev))\n",
    "                    if method in [\"diff_from_max_absolute_consecutive_frame_diff\", \"max_absolute_consecutive_frame_diff\"]:\n",
    "                        max_absolute_consecutive_frame_diff = get_absolute_consec_frame_diff_feature(video_path, max_consec_frame_diff=True)\n",
    "                    if method in [\"max_absolute_all_frame_diff\"]:\n",
    "                        absolute_all_frame_diff = get_absolute_all_frame_diff_feature(video_path, max_feature=True)\n",
    "                    if method in [\"min_absolute_all_frame_diff\"]:\n",
    "                        absolute_all_frame_diff = get_absolute_all_frame_diff_feature(video_path, max_feature=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    if video not in (test_video + val_video): \n",
    "                        #print(video)\n",
    "                        train=True\n",
    "                    else:\n",
    "                        train=False\n",
    "                        count_test = 0\n",
    "                        if video in test_video:\n",
    "                            testfolder_video = os.path.join(testfolder, video)\n",
    "                        else:\n",
    "                            testfolder_video = os.path.join(valfolder, video)\n",
    "                    \n",
    "                        \n",
    "                    if method in [\"optical_flow\", \"optical_flow_median_back\", \"optical_flow_from_median_frame\", \n",
    "                                  \"optical_flow_combined\", \"diff_from_max_absolute_consecutive_frame_diff\", \n",
    "                                  \"max_absolute_consecutive_frame_diff\", \"min_absolute_all_frame_diff\", \n",
    "                                  \"max_absolute_all_frame_diff\"]:\n",
    "                        #print(method)\n",
    "                        method_flow = cv2.calcOpticalFlowFarneback\n",
    "                        #params = [0.5, 3, 15, 3, 5, 1.2, 0]  # default Farneback's algorithm parameters\n",
    "                        # params = [0.5, 4, 18, 3, 5, 1.2, 0]  \n",
    "                        #params = [0.5, 2, 18, 3, 5, 1.2, 0]  \n",
    "                        #params = [0.5, 2, 20, 3, 5, 1.2, 0]  \n",
    "                        \n",
    "                        background = get_background(video_path, mean, sample=sample)\n",
    "                        # convert the background model to grayscale format\n",
    "                        background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "                        \n",
    "                        if method == \"optical_flow_from_median_frame\":\n",
    "                            frames_org, frames_optical_flow = gen_dense_optical_flow_data(method_flow, video_path, params, to_gray=True, median=True, median_frame=background)\n",
    "                        else :\n",
    "                            frames_org, frames_optical_flow = gen_dense_optical_flow_data(method_flow, video_path, params, to_gray=True,num_frames_prior=optical_flow_prior)\n",
    "                            \n",
    "                        if method == \"optical_flow_combined\":\n",
    "                            m_frames_org, m_frames_optical_flow = gen_dense_optical_flow_data(method_flow, video_path, params, to_gray=True, median=True, median_frame=background)\n",
    "                            c_frames_org, c_frames_optical_flow = gen_dense_optical_flow_data(method_flow, video_path, params, to_gray=True)\n",
    "    #                     print(len(frames_org))\n",
    "    #                     print(frames_org[0].shape)\n",
    "    #                     print(len(frames_optical_flow))\n",
    "    #                     print(frames_optical_flow[0].shape)\n",
    "    \n",
    "                        if method in [\"optical_flow\", \"optical_flow_from_median_frame\"]:\n",
    "                            pure = True\n",
    "                        else:\n",
    "                            pure = False\n",
    "                                              \n",
    "\n",
    "                        print(len(frames_org))        \n",
    "                        for i, frame in enumerate(frames_org):\n",
    "                            #save frame in images\n",
    "                            if train==True:\n",
    "                                image_dest = dest_dir + \"/\" +trainfolder +\"/\"\n",
    "                                count = count_train\n",
    "                            else:\n",
    "                                #print(video)\n",
    "                                image_dest = dest_dir +\"/\" + testfolder_video + \"/\"\n",
    "                                count = count_test\n",
    "\n",
    "                            #print(frame.shape)\n",
    "                            img = Image.fromarray(frame, \"RGB\")\n",
    "                            img.save(image_dest +\"images/\"+str(count)+\".tif\")\n",
    "                            if method == \"optical_flow_combined\":\n",
    "                                feature_image = create_feature_image_optical_flow(frame, c_frames_optical_flow[i], pure=pure, optical_flow2=m_frames_optical_flow[i])\n",
    "                            elif method == \"diff_from_max_absolute_consecutive_frame_diff\":\n",
    "                                diff_from_max_absolute_consecutive_frame_diff = get_diff_from_absolute_consec_frame_diff_feature(frame_diff_feature=max_absolute_consecutive_frame_diff, image_path=frame, frame=True)\n",
    "                                feature_image = create_feature_image_optical_flow(frame, frames_optical_flow[i], pure, diff_from_max_absolute_consecutive_frame_diff, final_channel=True)\n",
    "                            elif method == \"max_absolute_consecutive_frame_diff\":\n",
    "                                feature_image = create_feature_image_optical_flow(frame, frames_optical_flow[i], pure, max_absolute_consecutive_frame_diff, final_channel=False)\n",
    "                            elif method in [\"max_absolute_all_frame_diff\", \"min_absolute_all_frame_diff\"]:\n",
    "                                feature_image = create_feature_image_optical_flow(frame, frames_optical_flow[i], pure, absolute_all_frame_diff[i+1], final_channel=False)\n",
    "                            else:\n",
    "                                feature_image = create_feature_image_optical_flow(frame, frames_optical_flow[i], pure, background)\n",
    "                            frame_num = i+1\n",
    "                            \n",
    "                            count_test, count_train = save_data(image_dest, count, feature_image, frame_num, data_dir, video, minivideo, count_test, \n",
    "                                                                                count_train, train)\n",
    "                            if train == False:\n",
    "                                if video in test_video:\n",
    "                                    count = count_test_all\n",
    "                                    image_dest_all = dest_dir +\"/\" + testfolder + \"/\"\n",
    "                                    count_test_all, count_train = save_data(image_dest_all, count, feature_image, frame_num, data_dir, video, minivideo, count_test_all, \n",
    "                                                                                 count_train, train)\n",
    "                                else:\n",
    "                                    count = count_val_all\n",
    "                                    image_dest_all = dest_dir +\"/\" + valfolder + \"/\"\n",
    "                                    count_val_all, count_train = save_data(image_dest_all, count, feature_image, frame_num, data_dir, video, minivideo, count_val_all, \n",
    "                                                                                 count_train, train)\n",
    "                                img.save(image_dest_all +\"images/\"+str(count)+\".tif\")\n",
    "                                \n",
    "                            \n",
    "                    else:\n",
    "                        # print(\"in else\")\n",
    "                        # print(method)\n",
    "                        for frame in natsorted(os.listdir(os.path.join(data_dir, video, minivideo, \"images\"))):\n",
    "                            frame_num = int(frame.split(\".tif\")[0])\n",
    "                            #skip first frame\n",
    "                            if frame_num not in skip_frame_list:\n",
    "                                #save frame in images\n",
    "                                images_source = os.path.join(data_dir, video, minivideo, \"images\", frame)\n",
    "                                if train==True:\n",
    "                                    image_dest = dest_dir + \"/\" +trainfolder +\"/\"\n",
    "                                    count = count_train\n",
    "                                    #print(count)\n",
    "                                else:\n",
    "                                    #print(video)\n",
    "                                    image_dest = dest_dir +\"/\" + testfolder_video + \"/\"\n",
    "                                    image_dest_all = dest_dir +\"/\" + testfolder + \"/\"\n",
    "                                    count = count_test\n",
    "\n",
    "                                shutil.copy(images_source, image_dest +\"images/\"+str(count)+\".tif\")\n",
    "\n",
    "                                #create new image\n",
    "                                prev_frame = str(frame_num-1) +\".tif\"\n",
    "                                prev_image = os.path.join(data_dir, video, minivideo, \"images\", prev_frame) \n",
    "                                if method == \"background\":\n",
    "                                    feature_image = create_feature_image(image_path=images_source, background=background,prev_image_path=prev_image)\n",
    "                                if method == \"nprev\":\n",
    "                                    background=None\n",
    "                                    prev_frame2 = str(frame_num-num_prev) +\".tif\"\n",
    "                                    prev_image2 = os.path.join(data_dir, video, minivideo, \"images\", prev_frame2) \n",
    "                                    feature_image = create_feature_image(image_path=images_source, background=background, \n",
    "                                                                     prev_image_path=prev_image, prev_image_path2=prev_image2)\n",
    "\n",
    "\n",
    "\n",
    "                                count_test, count_train = save_data(image_dest, count, feature_image, frame_num, data_dir, video, minivideo, count_test, count_train, train)\n",
    "                                if train == False:\n",
    "                                    if video in test_video:\n",
    "                                        count = count_test_all\n",
    "                                        image_dest_all = dest_dir +\"/\" + testfolder + \"/\"\n",
    "                                        count_test_all, count_train = save_data(image_dest_all, count, feature_image, frame_num, data_dir, video, minivideo, count_test_all, \n",
    "                                                                                     count_train, train)\n",
    "                                    else:\n",
    "                                        count = count_val_all\n",
    "                                        image_dest_all = dest_dir +\"/\" + valfolder + \"/\"\n",
    "                                        count_val_all, count_train = save_data(image_dest_all, count, feature_image, frame_num, data_dir, video, minivideo, count_val_all, \n",
    "                                                                                     count_train, train)\n",
    "                                    shutil.copy(images_source, image_dest_all +\"images/\"+str(count)+\".tif\")\n",
    "                                    \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081d5ad-9786-43f5-a51a-d00c2ba9b70d",
   "metadata": {},
   "source": [
    "# Final data Training with easy and hard "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558a7b0",
   "metadata": {},
   "source": [
    "Update path variables and choose feature generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbbdc320-c10b-43e5-a6b3-4b664b1fa89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data_sub_dir = \"/home/medha/BacteriaDetectionTracking/MEMTrack/data/collagen/\" #path to  data dir containing loaded video\n",
    "dest_sub_dir = \"/home/medha/BacteriaDetectionTracking/MEMTrack/DataFeatures/\" # path feature dir\n",
    "exp_name = \"collagen_motility_inference\" # experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725c4c12-72fc-480c-9415-388c4de418b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optical_flow_median_back orginial\n",
    "#optical flow median back with optical_Flow_prior is optical flow from xth previous frame\n",
    "# \"diff_from_max_absolute_consecutive_frame_diff\" creates a feature for diff from the \"max consecutive frame diff\" feature, with a frame diff prior for xth frame diff\n",
    "\n",
    "feature_method = \"optical_flow_median_back\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de5cc4-bedc-4f03-98c7-242d4f57de5c",
   "metadata": {},
   "source": [
    "Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867460a3-28e6-412a-a759-1f458d8569fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = target_data_sub_dir\n",
    "dest_dir = os.path.join(dest_sub_dir, exp_name, \"data_feature_optical_flow_median_back_2pyr_18win_background_img/\")\n",
    "trainfolder = \"train\"\n",
    "testfolder = \"test\"\n",
    "valfolder = \"val\"\n",
    "\n",
    "#control, control1, 01, 02, 2_1, 2_2, 4_1, 4_2, 5_1, 5_2, 7_1, 7_2\n",
    "#O_5, 2_6, 4_6, 7_4, 7_7, 7_8\n",
    "\n",
    "train_video = [\"video217\", \"video218\",\"video222\",\"video223\", \"video229\",\"video230\", \"video236\", \"video237\",\n",
    "               \"video242\", \"video243\",\"video247\", \"video248\", \n",
    "               \"video226\",\"video234\", \"video241\",\"video250\",\"video253\",\"video254\"]\n",
    "               \n",
    "#control_2, 2_3, 4_3, 5_3, 7_3, o_3\n",
    "val_video = [\"video219\", \"video224\", \"video231\", \"video238\", \"video244\", \"video249\"]\n",
    "\n",
    "#Control3, O_6, 2_4, 4_4, 7_6, 5_4\n",
    "test_video = [\"video220\", \"video227\", \"video232\", \"video239\", \"video245\", \"video252\"]\n",
    "\n",
    "# Control 4, RBS O_4, RBS O_7, RBS 2_5, RBS 2_7, RBS 4_5, RBS 5_5, RBS 7_5, RBS 7_9, RBS 7_10\n",
    "\n",
    "final_test_options = [\"video221\", \"video225\", \"video228\", \"video233\", \"video235\", \n",
    "                      \"video240\", \"video246\", \"video251\", \"video255\", \"video256\"]\n",
    "test_video+=final_test_options\n",
    "\n",
    "params = [0.5, 2, 18, 3, 5, 1.2, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8091b86b-38bd-4150-84eb-7550231e8fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_data(data_dir, dest_dir, trainfolder, train_video, testfolder, test_video, valfolder, val_video, \n",
    "            method=feature_method, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22d75e",
   "metadata": {},
   "source": [
    "Creating Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d2d5b3c-1ffe-49c6-92f2-894c309eb419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video2\n",
      "/home/medha/BacteriaDetectionTracking/MEMTrack/data/collagen/video2/frame1video.mp4\n",
      "150\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "test_video_main = [\"video2\"]\n",
    "for video in test_video_main:\n",
    "    print(video)\n",
    "    data_dir = target_data_sub_dir\n",
    "    dest_dir = os.path.join(dest_sub_dir, exp_name , f\"data_{video}_feature_optical_flow_median_back_2pyr_18win_background_img/\")\n",
    "    trainfolder = \"train\"\n",
    "    testfolder = \"test\"\n",
    "    valfolder = \"val\"\n",
    "    val_video = []\n",
    "    test_video = [f\"{video}\"]\n",
    "    params = [0.5, 2, 18, 3, 5, 1.2, 0]  \n",
    "\n",
    "    create_data(data_dir, dest_dir, trainfolder, train_video, testfolder, test_video, valfolder, val_video, \n",
    "                method=feature_method, test_only = True, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e41de8-9a2f-4756-a582-efa7682870ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
